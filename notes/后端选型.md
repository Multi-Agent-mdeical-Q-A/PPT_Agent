# 模块二：后端 (Backend)

## 1) 候选架构对比：FastAPI vs Node.js vs Go/Rust vs 分层编排（以及为什么选 FastAPI）

### FastAPI（你当前选型）
**优点**
- **异步 I/O + WebSocket 一体化**：适合“音频分片上行、字幕/音频分片下行”的双向流式链路。
- **开发速度快**：你研究原型需要快速迭代（更换 ASR/TTS/LLM 组件、调中断策略、加日志）。
- **生态适合 ML 编排**：Python 更容易接各种 ASR/TTS 推理、GPU 推理框架、以及你后续的 RAG/多模态模块。
- **工程边界清晰**：FastAPI 做“会话/状态/协议/编排”，把重算子（ASR、TTS）放到 worker 或外部服务里，结构非常自然。
**缺点**
- 单进程 CPU 并发能力一般（但可通过多 worker + 任务队列/外部推理服务解决）。
- 如果把所有推理都堆在 API 进程里，容易被 GPU/CPU 卡死——因此必须做好“编排层与推理层拆分”。

### Node.js（NestJS/WS）
**优点**
- WebSocket、流式协议很顺手
- 单机网络吞吐通常不错
**缺点**
- 你后面做 RAG / 视觉理解 / 推理编排，Python 生态优势更大
- 在“模型推理 + 业务编排”一体化上，Python 更省胶水成本

### Go / Rust（高性能实时服务）
**优点**
- 性能、并发、内存占用优秀
- 适合做“媒体网关/会话网关/路由”
**缺点**
- 你原型阶段会频繁改模型链路与策略，Go/Rust 迭代速度更慢
- ML/推理集成成本更高（除非你完全服务化推理）

### 结论（推荐的实际落地）
✅ **FastAPI 最适合做“编排与会话层（Orchestrator）”**：
- WebSocket/状态机/中断控制/日志与指标统一在 FastAPI
- ASR/TTS/LLM 推理通过“可替换适配器”接入（本地推理或云 API 或独立推理服务）
---

## 2) 版本演进路线 (Roadmap)：v0.1（MVP）→ v1.0（完整版）

### v0.1（MVP：先跑通实时对话闭环）
**目标**：能说、能听、能打断
- WebSocket 单通道协议：
    - 上行：`audio_chunk`（或先用 `text_input` 简化）
    - 下行：`assistant_text`（一次性）+ `tts_audio`（一次性/短音频）
- 会话状态机：`idle/listening/thinking/speaking`
- 中断：收到 `interrupt` 立即停止 TTS 输出并通知前端停播

### v0.2（真正“实时”：流式 ASR + 流式 LLM + 流式 TTS）
- ASR：支持 partial（增量转写）
- LLM：token 流式输出（字幕实时出现）
- TTS：音频分片流式输出（边合成边播）
- 中断升级：支持 **barge-in**（用户一开口就打断数字人）

### v0.3（稳定性工程：可用、可复现、可观测）
- 断线重连：session 可恢复（短期状态放 Redis）
- 关键链路超时与降级：ASR/LLM/TTS 任一失败时有 fallback（比如仅文字回复、或短句 TTS）
- 日志与回放：记录时间轴（ASR 片段、LLM token、TTS chunk、interrupt 点）

### v0.5（接入课件上下文，但保持架构不变）
- 引入 `context_update`：前端发送当前页/当前段的文本（先不做 ROI）
- 后端把 context 注入 LLM prompt（或 RAG 检索）生成更贴 PPT 的讲解

### v1.0（完整版：可扩展、多会话、接近产品级）
- 推理服务化：ASR/TTS/LLM 可独立扩缩（GPU 资源池）
- 多协议支持：WebSocket（控制/字幕）+ WebRTC（音频媒体）可选
- 资源治理：并发限制、队列、优先级（讲解优先/后台任务降级）
- 指标体系完善：端到端延迟、首 token 延迟、首音频延迟、打断响应时间
---

## 3) 详细选型与落地：具体到库与技术组合逻辑

### 3.1 后端整体分层（强推荐）
把后端拆成三层，你后面怎么换模型都不影响主干：
1. **Gateway / API（FastAPI）**
- WebSocket 会话、协议、鉴权、状态机
- 不做重推理，最多做轻量 VAD/路由
2. **Orchestrator（编排器，可仍在 FastAPI 内部实现，也可拆服务）**
- 把 ASR → LLM → TTS 串成可中断的流式管线
- 负责“打断时取消哪些任务、清哪些缓冲、发哪些状态”
3. **Workers（推理层）**
- ASR worker / TTS worker /（可选）LLM worker
- 可以是本地库调用，也可以是 HTTP/gRPC 调用外部推理服务
> 逻辑：FastAPI 负责“连接与协议”；推理层负责“算力”；编排器负责“体验”。
---

### 3.2 FastAPI 推荐库组合
- **FastAPI + Uvicorn**：基础服务
- **WebSocket（Starlette 内置）**：双向流（适合音频 chunk 与控制信令）
- **Pydantic**：消息 schema（强烈建议所有 WS 消息都有 type + session_id + seq）
- **Redis（可选但强推荐）**：会话状态/短期缓存/断线恢复/队列
- **结构化日志**：loguru 或 structlog（方便做“时间轴回放”）
- **异步队列**：`asyncio.Queue`（v0.1/v0.2 足够）；规模上来再换 Redis Stream/NATS
---

### 3.3 实时流式链路的“关键实现点”

#### 音频上行（用户 → ASR）
- 输入格式：建议统一为 **16kHz mono PCM**（原型阶段最省事）
- chunk：20ms～40ms 一片（太大延迟高，太小开销大）
- VAD（语音活动检测）：
    - v0：简单能量阈值（快）
    - v0.3：接入更稳的 VAD（减少误打断）

#### LLM 流式输出（字幕）
- 后端向前端推：`assistant_text_delta`
- 同时维护一个“当前句缓冲”，用于触发 TTS（见下一条）

#### TTS 流式输出（数字人语音）
两种常见策略（你可以 v0/v1 分别用）：
- **策略 A：句级触发（更稳）**
    LLM 输出到句号/停顿 → 触发一次 TTS → 播放 chunk
    优点：简单、稳定；缺点：比极致流式慢一点
- **策略 B：真流式（更低延迟）**
    LLM token 来一点就送给 TTS（需要 TTS 支持 streaming）
    优点：最实时；缺点：工程更复杂，中断/回滚更难
> 建议：**v0.2 用句级触发，v1.0 再上真流式**。
---

### 3.4 中断（Barge-in）是后端设计的“第一优先级”
你要的是实时对话数字人，不是视频播放，所以中断必须是“硬中断”。
**推荐的中断机制（强实用）**
- 每个 session 有一个 `generation_id`（或 `turn_id`），每次开始说话就递增
- 所有下行消息都带 `generation_id`
- 当前端发 `interrupt`：
    1. 后端立刻：`generation_id += 1`
    2. 取消正在运行的 asyncio Tasks（ASR/LLM/TTS）
    3. 清空输出队列（tts_audio_queue、text_queue）
    4. 下行发 `state_update: idle` 给前端
- 前端收到新的 `generation_id` 之后，**丢弃旧 id 的任何迟到音频/字幕**（防止“打断后又冒出旧音频”）
这套机制能非常干净地解决：网络抖动/异步延迟导致的“幽灵音频”。
---

### 3.5 你和前端 Electron+WebSocket 的协议建议（最小但可扩展）
**上行**
- `start_listen` / `stop_listen`
- `audio_chunk`（含 seq、format）
- `interrupt`
- `ui_event`（翻页、点击、指向，v0 可以先不接）
**下行**
- `asr_partial` / `asr_final`
- `assistant_text_delta`
- `tts_audio_chunk`（含 codec、sample_rate、seq、generation_id）
- `state_update`（listening/thinking/speaking/idle）
- `error`（可恢复/不可恢复）
---

## 4) 架构扩展性：未来如何平滑升级（性能/实时/多会话/多渲染方案）
你现在先定前后端架构没问题，后端扩展主要看这 4 个方向：

### 4.1 从 WebSocket 音频升级到 WebRTC（体验显著提升）
- WebSocket 方案适合 v0 快跑
- v1.0 若要更接近产品体验：媒体走 WebRTC（回声消除、抖动缓冲、丢包恢复更强），WebSocket 只保留控制/字幕

### 4.2 推理服务化（把 GPU 负载从 API 进程挪走）
- ASR/TTS/LLM 独立为 worker（HTTP/gRPC）
- FastAPI 只做“连接与编排”
- 好处：并发上来不会把 WebSocket 网关拖死

### 4.3 统一“驱动协议”，支撑未来 Live2D → 3D/神经渲染
虽然这是前端/数字人模块的事，但后端要提前铺路：
- 后端下行除了 `tts_audio_chunk`，未来可加：
    - `viseme_timeline`（音素嘴型时间轴）
    - `emotion_tag`（情绪/语气标签）
    - `blendshape_frame`（如果未来接 3D/Audio2Face）
- 这样你不改主链路，只是多发一种“驱动信号”。

### 4.4 可观测与实验评估（研究原型非常重要）
后端最好从 v0.3 开始就记录这些指标：
- `t_user_end → first_asr_partial`
- `t_user_end → first_token`
- `t_user_end → first_audio`
- interrupt 响应耗时（从 interrupt 到停止推送音频）
- 每轮 token 数、TTS 音频时长、ASR 错误率（至少日志里有）
