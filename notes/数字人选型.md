> 你的阶段目标是“实时对话 + PPT 同屏”，而不是追求照片级渲染。因此数字人模块的核心指标是：**低算力、低延迟、可打断、易集成、可扩展**。Live2D（Cubism）非常符合这一组合。

## 1) 候选架构对比：Live2D vs 其他 2D vs Talking Head（以及为什么选 Live2D）

### A. Live2D（参数化 2D 模型，推荐）
**优点**
- **实时驱动天然友好**：本质就是“给参数赋值（0~1）+ 每帧更新”，非常适合从音频实时驱动嘴型。官方手册明确了可以在模型 update 前通过 `setParameterValue/addParameterValue` 直接控制口型开合度（0~1）。[docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/lipsync/?utm_source=chatgpt.com)
- **模型自带参数分组（更通用）**：很多 `.model3.json` 会包含 `Groups`，例如 LipSync/ EyeBlink 对应的参数 ID（如 `ParamMouthOpenY`, `ParamEyeLOpen`, `ParamEyeROpen`），这让你不必“写死参数名”，而是**自动适配不同模型**。[docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/autoeyeblink/?utm_source=chatgpt.com)
- **渲染轻量**：对显存/算力压力远低于 UE + MetaHuman，适合 Electron 常驻运行。
- **生态成熟**：Cubism SDK for Web 本身就是面向 Web/TypeScript 的开发套件，与你 Electron（Chromium）技术底座吻合。[docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/cubism-sdk-for-web/?utm_source=chatgpt.com)
**缺点**
- “真实感上限”不如神经渲染/3D 高端方案（但你已明确当前不关注表现，这是可接受的）。

### B. 其他 2D（Spine / Rive / Lottie）
**优点**
- 工程更轻、资源制作成熟（尤其 Rive/Lottie）。
- 适合做“状态机动画”（idle/listening/thinking/speaking）很快出效果。
**缺点**
- 口型通常更“假”（多靠循环 talk 动画或简单开合），缺少 Live2D 那种“参数驱动的连续可控性”。
![[Pasted image 20251222153058.png|500]]
![[Pasted image 20251222153106.png|500]]

### C. Talking Head（照片/视频驱动的说话头）
**优点**
- 视觉真实感更强（更像真人/主播）。
**缺点（与你当前阶段冲突）**
- 很多实现偏“生成视频片段”，要做低延迟流式交互更难；工程复杂度高、打断处理更麻烦。
- 对你当前研究重点（AI 生成讲解内容）投入产出不划算。
✅ **结论**：当你把“实时、轻量、可打断、可扩展”作为第一优先级时，Live2D 是最合适的 v0 数字人实现方案。
---

## 2) 版本演进路线（Roadmap）：v0.1（能说能动）→ v1.0（可扩展表达层）

### v0.1（MVP：最小可用的实时对话数字人）
目标：**能显示、能动嘴、能打断**
- 渲染：加载 1 个 Live2D 模型并显示（固定位置/缩放）
- 嘴型：**音频能量驱动 mouthOpen**（不做 viseme）
- 状态：idle / speaking 两态（说话=动嘴+轻微摆动）
- 打断：收到后端 `interrupt` 或本地检测到用户开始说话 → 立刻停止嘴型驱动并切回 idle

### v0.2（体验增强：更像“实时助教”）
- 四态状态机：idle / listening / thinking / speaking
- 自动眨眼：接入 eyeBlink（随机间隔）能力（如果模型支持 EyeBlink 组）[docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/autoeyeblink/?utm_source=chatgpt.com)
- 视线/头部跟随：轻量 gaze（随鼠标或随 PPT 焦点区域移动）

### v0.3（可控性增强：从“能量驱动”升级为“viseme 驱动”）
- 后端/前端产生 **viseme timeline**（音素嘴型时间轴）
- Live2D 参数从 “单一 mouthOpen” 升级为 “mouthOpen + mouthForm（若模型支持）”
- 支持“语气/情绪标签”影响表情（微笑、疑惑、强调）

### v0.5（可复用与可替换：统一驱动协议）
- 抽象 `AvatarDriver`：输入统一为 `audio` / `viseme` / `emotion` / `state`
- Live2D 只是一个实现（Live2DDriver），未来替换 3D/神经渲染不动上层逻辑

### v1.0（为后续高级渲染铺路）
- 驱动信号标准化（后端可输出：viseme、表情标签、甚至 blendshape）
- 前端支持切换渲染器：Live2D / 轻量 3D / 视频流头像（按能力逐步解锁）
---

## 3) 详细选型与落地：驱动策略、参数设计、状态机、以及技术栈组合逻辑

### 3.1 推荐实现组合（Electron 里最常见、最省心）
- **渲染层**：PixiJS + Live2D 显示插件
    `pixi-live2d-display` 是社区里很常见的 Web 端 Live2D 集成方案，支持多个 Cubism 版本（2.1/3/4 等），并强调需要引入 Cubism Core（运行时）。[GitHub](https://github.com/guansss/pixi-live2d-display?utm_source=chatgpt.com)
- **驱动层**：WebAudio（AnalyserNode）做音频能量 → 参数映射（v0.1）
- **状态机**：前端 Pinia（或你现有状态管理）维护 avatar state，并用 WebSocket 接收后端状态
> 这样做的好处是：**数字人驱动与后端 ASR/LLM/TTS 完全解耦**。v0 甚至可以“只接收音频流”就能动嘴。
---

### 3.2 驱动策略（按你“先不卷表现”的优先级排序）

#### 策略 A：音频能量驱动（v0 首选）
- 输入：正在播放的 TTS 音频（前端本地即可拿到）
- 处理：实时计算 RMS/能量，归一化为 0~1
- 输出：写入 LipSync 参数（通常是 `ParamMouthOpenY` 或者从 Groups 自动读取）
- 原理与官方一致：手册明确了可以用“实时音量值”直接控制嘴巴开合度，并在 update 前写入参数值。[docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/lipsync/?utm_source=chatgpt.com)
✅ 优点：实现最快、天然同步、打断最干净
⚠️ 缺点：口型不区分音素（但你当前不关注表现）

#### 策略 B：Viseme/音素时间轴驱动（v0.3 开始）
- 输入：后端 TTS 返回的音素/viseme time-alignment（或你自行对齐）
- 输出：随时间写入 mouthOpen、mouthForm 等多个参数（若模型支持）
- 优点：口型更准、可控性更强（强调/停顿更自然）
- 缺点：需要 TTS 或对齐模块支持 time-alignment（工程复杂度更高）
---

### 3.3 参数设计（“通用映射”比“写死 Param 名”更重要）

#### 推荐原则：优先从模型 `.model3.json` 的 Groups 自动读取
Live2D 的自动眨眼文档展示了 `.model3.json` 中常见分组结构：
- LipSync：常见 `ParamMouthOpenY`
- EyeBlink：常见 `ParamEyeLOpen`, `ParamEyeROpen` [docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/autoeyeblink/?utm_source=chatgpt.com)
这样你就能做到：
- 换模型不改代码（最关键）
- 同一个驱动逻辑适配多种角色资产

#### v0 建议你至少支持这三类参数（常见且收益最大）
1. **LipSync**：mouth open（必做）
2. **EyeBlink**：左右眼开合（建议 v0.2 做）[docs.live2d.com](https://docs.live2d.com/zh-CHS/cubism-sdk-manual/autoeyeblink/?utm_source=chatgpt.com)
3. **Pose/Angle**：头部轻微摆动（提升“活着感”，可用 idle motion 解决）
> 注意：不同模型参数 ID 可能不完全一致，所以“从 Groups/SettingJson 读取”比硬编码更可靠。
---

### 3.4 状态机（Avatar FSM）：把“对话体验”变成确定行为
你可以把 Live2D 视为一个简单的有限状态机，状态由后端/前端事件驱动：
- **idle**：呼吸/轻微摆动（Loop motion）
- **listening**：轻微前倾/专注表情（不动嘴）
- **thinking**：小幅度头动/眨眼频率变化（不动嘴）
- **speaking**：嘴型驱动开启 + 说话 motion（可选）
**事件触发**
- `USER_SPEAK_START`：进入 listening，并发送 `interrupt`（打断数字人）
- `ASR_FINAL`：进入 thinking
- `LLM_FIRST_TOKEN`：可保持 thinking 或转 speaking（看你是否先字幕再出声）
- `TTS_AUDIO_START`：进入 speaking
- `TTS_AUDIO_END`：回 idle
- `INTERRUPT`：立即停止嘴型驱动 + 清空队列 + 回 idle/listening
> 核心是：**interrupt 具有最高优先级**，任何状态都能被打断并立即停止输出。
---

## 4) 后续升级路线：从 Live2D 平滑升级到更高端渲染
你在模块一里已经提到“未来可升级渲染方案”，数字人模块这里建议把升级拆成“只换渲染器，不换驱动协议”。

### 4.1 先把“驱动协议”标准化（最关键）
无论 Live2D / 3D / 神经渲染，你都可以统一对外暴露：
- `setState(state)`
- `driveAudioLevel(x)` 或 `driveViseme(timeline)`
- `setEmotion(tag|valence)`
- `interrupt()`
这样未来升级只需要新增一个实现：
- `Live2DRenderer`
- `VRMRenderer（Three.js）`
- `BlendshapeRenderer（Audio2Face/ARKit 系数）`
- `VideoAvatarRenderer（流式帧/视频纹理）`

### 4.2 升级阶梯（建议顺序）
1. **Live2D（能量驱动） → Live2D（viseme 驱动）**：表现提升，代价适中
2. **Live2D → 轻量 3D（VRM + blendshape）**：进入 3D 表达，但仍可低算力
3. **轻量 3D → 实时面部动画引擎输出（ARKit blendshape）**：你第三模块要的“唇形同步/表情”研究空间更大
4. **神经渲染头像（更远期）**：在不推翻系统的前提下替换渲染器
