from typing import List
from .state import AgentState
from ..config.config_loader import settings
# å¼•å…¥æ–°çš„ PromptBuilder
from .prompt_builder import PromptBuilder

class CragNodes:
    def __init__(self, tools):
        self.tools = tools

    def evaluate_node(self, state: AgentState) -> AgentState:
        """
        [Node 1] è£åˆ¤èŠ‚ç‚¹
        """
        print("ğŸ¤” [Node] Evaluating retrieval quality...")
        
        all_scores = []
        # é€ä¸ªé—®é¢˜å¤„ç†
        for _id, q, docs in zip(state.ids, state.queries, state.raw_docs):
            
            # ã€ä¼˜åŒ–ç‚¹ã€‘
            # å¦‚æœ docs ä¸ºç©º (æ¯”å¦‚æŸäº›æ•°æ®æºç¼ºå¤±)ï¼Œè¿™é‡Œä¼šå¯¼è‡´ q_repeated ä¸ºç©ºï¼Œevaluator æŠ¥é”™ã€‚
            # åŠ ä¸€ä¸ªç®€å•çš„é˜²å¾¡
            if not docs:
                all_scores.append([0.0] * 10) # å¡«å……é»˜è®¤ä½åˆ†
                continue

            q_repeated = [q] * len(docs)
            id_repeated = [str(_id)] * len(docs)
            
            # è°ƒç”¨ EvaluatorTool
            scores = self.tools['evaluator'].run_pair(q_repeated, docs, ids=id_repeated)
            all_scores.append(scores)
            
        state.scores = all_scores
        return state

    def decide_node(self, state: AgentState) -> AgentState:
        """
        [Node 2] å†³ç­–èŠ‚ç‚¹
        """
        print("âš–ï¸ [Node] Making decisions (Correct/Ambiguous/Incorrect)...")
        upper = settings.params['upper_threshold']
        lower = settings.params['lower_threshold']
        
        flags = []
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ state.scores å’Œ queries é•¿åº¦ä¸€è‡´
        for scores in state.scores:
            doc_flags = []
            for s in scores:
                if s >= upper: doc_flags.append(2)
                elif s >= lower: doc_flags.append(1)
                else: doc_flags.append(0)
            
            if 2 in doc_flags: 
                final_flag = "internal"
            elif 1 in doc_flags: 
                final_flag = "combined"
            else: 
                final_flag = "external"
            
            flags.append(final_flag)
            
        state.flags = flags
        return state

    def refine_node(self, state: AgentState) -> AgentState:
        """
        [Node 3] æ‰§è¡ŒèŠ‚ç‚¹ (Mock Retrieval)
        """
        print("âœ‚ï¸ [Node] Refining knowledge (Mock Retrieval)...")
        
        contexts = []
        for _id, flag in zip(state.ids, state.flags):
            res = self.tools['refiner'].run([_id], type=flag)
            if res and len(res) > 0:
                contexts.append(res[0])
            else:
                contexts.append("") 
            
        state.final_contexts = contexts
        return state

    def generate_node(self, state: AgentState) -> AgentState:
        """
        [Node 4] ç”ŸæˆèŠ‚ç‚¹ï¼šç»„è£… Prompt å¹¶è°ƒç”¨ LLM
        """
        print("âœï¸ [Node] Generating answers...")
        prompts = []
        
        # 1. è·å–åŠ¨æ€é…ç½®å‚æ•°
        # ä» settings è¯»å– task (popqa/pubqa)
        current_task = settings.task_name
        if not current_task:
            current_task = 'popqa' # å…œåº•é»˜è®¤å€¼
            
        # ã€æ–°å¢ã€‘ä» settings è¯»å– model_type (selfrag/llama)
        # è¿™è§£å†³äº†ä½ æåˆ°çš„â€œä¸è¦å†™æ­»â€çš„é—®é¢˜
        gen_type = settings.models.get('generator_type', 'llama')

        # 2. ç»„è£… Batch Prompts
        for q, ctx in zip(state.queries, state.final_contexts):
            
            # ä½¿ç”¨ PromptBuilder å·¥å‚æ„å»º Prompt
            # è¿™é‡Œçš„ model_name å‚æ•°ç°åœ¨æ˜¯åŠ¨æ€çš„äº†
            prompt = PromptBuilder.build(
                task=current_task, 
                question=q, 
                context=ctx,
                model_name=gen_type 
            )
            prompts.append(prompt)
            
        # 3. æ‰¹é‡ç”Ÿæˆ
        str_ids = [str(i) for i in state.ids]
        answers = self.tools['generator'].run(prompts, ids=str_ids)
        
        state.final_answers = answers
        return state
